{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import wikipediaapi\n",
    "import pycountry_convert as pc\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download csv data into pandas dataframe\n",
    "cities_pop = pd.read_csv(\"worldcities.csv\")  \n",
    "\n",
    "# delete columns that definitely won't be used\n",
    "del cities_pop['lat']\n",
    "del cities_pop['lng']\n",
    "del cities_pop['iso3']\n",
    "del cities_pop['admin_name']\n",
    "del cities_pop['id']\n",
    "\n",
    "# print first 5 dataframe in the list to check\n",
    "cities_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add continets column\n",
    "cities_pop['continent'] = ''\n",
    "\n",
    "# For each row in population add the continent code\n",
    "for i in range(len(cities_pop)):\n",
    "    # hard fixes required for pycountry library to work\n",
    "    if (cities_pop['country'].iloc[i] == 'Gaza Strip'):\n",
    "        cities_pop.at[i, 'continent'] = 'AS'\n",
    "        continue; \n",
    "    if (cities_pop['country'].iloc[i] == 'West Bank'):\n",
    "        cities_pop.at[i, 'continent'] = 'AS'\n",
    "        continue; \n",
    "    if (cities_pop['country'].iloc[i] == 'Falkland Islands '):\n",
    "        cities_pop.at[i, 'continent'] = 'SA'\n",
    "        continue; \n",
    "    if (cities_pop['country'].iloc[i] == 'Kosovo'):\n",
    "        cities_pop.at[i, 'continent'] = 'NA'\n",
    "        continue; \n",
    "    if (cities_pop['country'].iloc[i] == 'Saint Barthelemy'):\n",
    "        cities_pop.at[i, 'continent'] = 'SA'\n",
    "        continue; \n",
    "    \n",
    "    country_code = pc.country_name_to_country_alpha2(cities_pop['country'].iloc[i], cn_name_format=\"default\")\n",
    "\n",
    "    # hard fixes required for pycountry library to work\n",
    "    if (country_code == 'TL'):\n",
    "        cities_pop.at[i, 'continent'] = 'AS'\n",
    "    elif (country_code == 'VA'):\n",
    "        cities_pop.at[i, 'continent'] = 'EU'\n",
    "    elif (country_code == 'SX'):\n",
    "        cities_pop.at[i, 'continent'] = 'SA'\n",
    "    elif (country_code == 'PN'):\n",
    "        cities_pop.at[i, 'continent'] = 'AS'\n",
    "    else:\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "        cities_pop.at[i, 'continent'] = continent_code\n",
    "\n",
    "# print first 5 dataframe in the list to check\n",
    "cities_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRS sample of size 100\n",
    "cities_srs = cities_pop.sample(n = 100)\n",
    "\n",
    "# Find proportional sizes for stratified sample \n",
    "N = len(cities_pop)\n",
    "N_NA = sum(cities_pop['continent'] == 'NA')\n",
    "N_SA = sum(cities_pop['continent'] == 'SA')\n",
    "N_EU = sum(cities_pop['continent'] == 'EU')\n",
    "N_AS = sum(cities_pop['continent'] == 'AS')\n",
    "N_AF = sum(cities_pop['continent'] == 'AF')\n",
    "N_OC = sum(cities_pop['continent'] == 'OC')\n",
    "\n",
    "n = 100\n",
    "n_NA = round(100 * (N_NA/N))\n",
    "n_SA = round(100 * (N_SA/N))\n",
    "n_EU = round(100 * (N_EU/N))\n",
    "n_AS = round(100 * (N_AS/N))\n",
    "n_AF = round(100 * (N_AF/N))\n",
    "n_OC = round(100 * (N_OC/N))\n",
    "\n",
    "# Stratified samples totalling in sample size = 100\n",
    "cities_NA = cities_pop[cities_pop['continent'] == \"NA\"].sample(n_NA)\n",
    "cities_SA = cities_pop[cities_pop['continent'] == \"SA\"].sample(n_SA)\n",
    "cities_EU = cities_pop[cities_pop['continent'] == \"EU\"].sample(n_EU)\n",
    "cities_AS = cities_pop[cities_pop['continent'] == \"AS\"].sample(n_AS)\n",
    "cities_AF = cities_pop[cities_pop['continent'] == \"AF\"].sample(n_AF)\n",
    "cities_OC = cities_pop[cities_pop['continent'] == \"OC\"].sample(n_OC)\n",
    "\n",
    "cities_strat = pd.concat([cities_NA, cities_SA, cities_EU, cities_AS, cities_AF, cities_OC], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>continent</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13378</th>\n",
       "      <td>Euclid</td>\n",
       "      <td>Euclid</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46550.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14119</th>\n",
       "      <td>Linton Hall</td>\n",
       "      <td>Linton Hall</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41386.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36241</th>\n",
       "      <td>Springs</td>\n",
       "      <td>Springs</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7036.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18727</th>\n",
       "      <td>Waukee</td>\n",
       "      <td>Waukee</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24089.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36726</th>\n",
       "      <td>Tabernacle</td>\n",
       "      <td>Tabernacle</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6851.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>2198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city   city_ascii        country iso2 capital  population  \\\n",
       "13378       Euclid       Euclid  United States   US     NaN     46550.0   \n",
       "14119  Linton Hall  Linton Hall  United States   US     NaN     41386.0   \n",
       "36241      Springs      Springs  United States   US     NaN      7036.0   \n",
       "18727       Waukee       Waukee  United States   US     NaN     24089.0   \n",
       "36726   Tabernacle   Tabernacle  United States   US     NaN      6851.0   \n",
       "\n",
       "      continent  word_count  \n",
       "13378        NA        2133  \n",
       "14119        NA         705  \n",
       "36241        NA         780  \n",
       "18727        NA         948  \n",
       "36726        NA        2198  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the wikipedia object\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# add word count column \n",
    "word_count_srs = []\n",
    "word_count_strat = []\n",
    "\n",
    "# loop through SRS, look up Wikipedia page, and get length of page\n",
    "for i in range(len(cities_srs)):\n",
    "    page_py = wiki_wiki.page(cities_srs['city_ascii'].iloc[i])\n",
    "    words = len(page_py.text.split())\n",
    "    word_count_srs.append(words)\n",
    "\n",
    "# loop through stratified sample, look up Wikipedia page, and get length of page\n",
    "for i in range(len(cities_strat)):\n",
    "    page_py = wiki_wiki.page(cities_strat['city_ascii'].iloc[i])\n",
    "    words = len(page_py.text.split())\n",
    "    word_count_strat.append(words)\n",
    "\n",
    "cities_srs['word_count'] = word_count_srs\n",
    "cities_strat['word_count'] = word_count_strat\n",
    "\n",
    "cities_srs.head(5)\n",
    "cities_strat.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays that will be added as columns to the df\n",
    "views_list = []\n",
    "views_list_s = []\n",
    "\n",
    "# send API query to Wikipedia asking for number of views the page has in the calendar year of 2022 for the SRS\n",
    "for i in range(len(cities_srs)):\n",
    "    city = cities_srs['city_ascii'].iloc[i].replace(\" \",\"_\")\n",
    "    url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/' + city + '/daily/2022010100/2022110700'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if(resp.status_code == 404):\n",
    "        views_list.append(0)\n",
    "        continue;\n",
    "    data = resp.json()\n",
    "    views_list.append(data['items'][0]['views'])\n",
    "\n",
    "\n",
    "# send API query to Wikipedia asking for number of views the page has in the calendar year of 2022 for the stratified sample\n",
    "for i in range(len(cities_strat)):\n",
    "    city = cities_srs['city_ascii'].iloc[i].replace(\" \",\"_\")\n",
    "    url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/' + city + '/daily/2022010100/2022110700'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if(resp.status_code == 404):\n",
    "        views_list_s.append(0)\n",
    "        continue;\n",
    "    data = resp.json()\n",
    "    views_list_s.append(data['items'][0]['views'])\n",
    "\n",
    "cities_srs['views'] = views_list\n",
    "cities_strat['views'] = views_list_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to two separate csv files\n",
    "srs_data = cities_srs.to_csv('cities_srs.csv', index = True)\n",
    "strat_data = cities_strat.to_csv('cities_strat.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d122254fe49901473e2f55449eb2edd109e321fcb25f1ff4c19f988cab42c2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
